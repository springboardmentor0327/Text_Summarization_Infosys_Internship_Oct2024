{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_rYrq7wUm0p",
        "outputId": "4637adf7-8083-4ac3-bdb2-4b35af0abb64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Install required libraries\n",
        "%pip install --quiet gradio transformers nltk numpy networkx sumy langchain-google-genai pypdf langchain-community\n",
        "\n",
        "# Import necessary libraries\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "# Download necessary NLTK data files\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qurc6I410Evf"
      },
      "outputs": [],
      "source": [
        "# Frequency function\n",
        "def frequency_based_summary(text, word_limit):\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text.lower())\n",
        "    freq_table = {word: words.count(word) for word in words if word not in stop_words}\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentence_scores = {sentence: sum(freq_table.get(word, 0) for word in word_tokenize(sentence.lower()))\n",
        "                       for sentence in sentences}\n",
        "\n",
        "    summary, word_count = [], 0\n",
        "    for sentence in sorted(sentence_scores, key=sentence_scores.get, reverse=True):\n",
        "        sentence_word_count = len(word_tokenize(sentence))\n",
        "\n",
        "        if sentence_word_count > word_limit * 0.5:\n",
        "            continue\n",
        "        if word_count + sentence_word_count <= word_limit:\n",
        "            summary.append(sentence)\n",
        "            word_count += sentence_word_count\n",
        "        else:\n",
        "            break\n",
        "    return \" \".join(summary)\n",
        "\n",
        "# LSA function\n",
        "def lsa_summary(text, word_limit):\n",
        "    try:\n",
        "        sentences = sent_tokenize(text)\n",
        "        if len(sentences) == 0:\n",
        "            return \"No content to summarize.\"\n",
        "\n",
        "        vectorizer = CountVectorizer()\n",
        "        X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "        svd = TruncatedSVD(n_components=1)\n",
        "        scores = svd.fit_transform(X).flatten()\n",
        "\n",
        "        ranked_sentences = [sentences[i] for i in np.argsort(scores)[::-1]]\n",
        "\n",
        "        summary = []\n",
        "        word_count = 0\n",
        "        for sentence in ranked_sentences:\n",
        "            sentence_word_count = len(word_tokenize(sentence))\n",
        "            if sentence_word_count > word_limit * 0.5:\n",
        "                continue\n",
        "            if word_count + sentence_word_count <= word_limit:\n",
        "                summary.append(sentence)\n",
        "                word_count += sentence_word_count\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return \" \".join(summary)\n",
        "    except Exception as e:\n",
        "        return f\"Error generating summary with LSA: {e}\"\n",
        "\n",
        "# LexRank function\n",
        "def lex_rank_summary(text, word_limit):\n",
        "    try:\n",
        "        sentences = sent_tokenize(text)\n",
        "        vectorizer = CountVectorizer().fit_transform(sentences)\n",
        "        tfidf_matrix = TfidfTransformer().fit_transform(vectorizer)\n",
        "        similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()\n",
        "        nx_graph = nx.from_numpy_array(similarity_matrix)\n",
        "        scores = nx.pagerank(nx_graph)\n",
        "        ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
        "        summary = []\n",
        "        word_count = 0\n",
        "        for _, sentence in ranked_sentences:\n",
        "            sentence_word_count = len(word_tokenize(sentence))\n",
        "            if word_count + sentence_word_count <= word_limit:\n",
        "                summary.append(sentence)\n",
        "                word_count += sentence_word_count\n",
        "            else:\n",
        "                break\n",
        "        return \" \".join(summary)\n",
        "    except Exception as e:\n",
        "        return f\"Error generating summary with LexRank: {e}\"\n",
        "\n",
        "# Luhn function\n",
        "def luhn_summary(text, word_limit):\n",
        "    from sumy.parsers.plaintext import PlaintextParser\n",
        "    from sumy.nlp.tokenizers import Tokenizer\n",
        "    from sumy.summarizers.luhn import LuhnSummarizer\n",
        "\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LuhnSummarizer()\n",
        "\n",
        "    preliminary_summary = summarizer(parser.document, sentences_count=len(text.split(\".\")))\n",
        "\n",
        "    summary = []\n",
        "    word_count = 0\n",
        "    for sentence in preliminary_summary:\n",
        "        sentence = str(sentence)\n",
        "        sentence_word_count = len(word_tokenize(sentence))\n",
        "        if word_count + sentence_word_count <= word_limit:\n",
        "            summary.append(sentence)\n",
        "            word_count += sentence_word_count\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return \" \".join(summary)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ubp7PVRDl1xL"
      },
      "outputs": [],
      "source": [
        "# T5 function\n",
        "def t5_summary(text, word_limit):\n",
        "    try:\n",
        "        t5_summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
        "        return t5_summarizer(text, max_length=word_limit, min_length=word_limit // 2, do_sample=False)[0]['summary_text']\n",
        "    except Exception as e:\n",
        "        return f\"Error generating summary with T5: {e}\"\n",
        "\n",
        "# fucntion to handle large files for BART\n",
        "def chunk_text(text, max_length=500):\n",
        "    tokens = word_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for token in tokens:\n",
        "        if len(current_chunk) + len(token) <= max_length:\n",
        "            current_chunk.append(token)\n",
        "        else:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = [token]\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# BART function\n",
        "def bart_summary(text, word_limit):\n",
        "    bart_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "    chunks = chunk_text(text, max_length=500)  # Split text into chunks of 500 tokens\n",
        "    summaries = []\n",
        "    summary = bart_summarizer(chunks, max_length=word_limit, min_length=word_limit // 2, do_sample=False)[0]['summary_text']\n",
        "    summaries.append(summary)\n",
        "    final_summary = \" \".join(summaries)\n",
        "    return final_summary\n",
        "\n",
        "# Large Language Models\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import Document\n",
        "\n",
        "def get_prompt_template():\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                \"Write a concise summary of the following in {num_words} words:\\\\n\\\\n\",\n",
        "            ),\n",
        "            (\"human\", \"{context}\")\n",
        "        ]\n",
        "    )\n",
        "    return prompt\n",
        "\n",
        "def llm_summary(text, word_limit):\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "      model=\"gemini-1.5-flash\",\n",
        "      temperature=0,\n",
        "      max_tokens=word_limit,\n",
        "      timeout=None,\n",
        "      max_retries=2\n",
        ")\n",
        "\n",
        "    prompt = get_prompt_template()\n",
        "    chain = prompt | llm\n",
        "\n",
        "    # Invoke chain\n",
        "    result = chain.invoke({\n",
        "        \"context\": text,\n",
        "        \"num_words\": word_limit\n",
        "    })\n",
        "\n",
        "    return result.content\n",
        "\n",
        "# Map reduce function\n",
        "def map_reduce_summary(text, word_limit):\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", max_tokens=word_limit,min_tokens=word_limit)\n",
        "    chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "    docs = [Document(page_content=text)]\n",
        "    summary = chain.invoke(docs)\n",
        "    return summary['output_text']\n",
        "\n",
        "# Iterative refinement function\n",
        "def iterative_refinement_summary(text, word_limit):\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", max_tokens=word_limit)\n",
        "    chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
        "    docs = [Document(page_content=text)]\n",
        "    summary = chain.invoke(docs)\n",
        "    return summary['output_text']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#summarization Logic\n",
        "def summarize(input_text, summarization_type, method, word_limit=None):\n",
        "    extractive_methods = {\n",
        "        \"Frequency-based\": frequency_based_summary,\n",
        "        \"Luhn\": luhn_summary,\n",
        "        \"LSA\": lsa_summary,\n",
        "        \"LexRank\": lex_rank_summary,\n",
        "    }\n",
        "    abstractive_methods = {\n",
        "        \"T5\": t5_summary,\n",
        "        \"BART\": bart_summary,\n",
        "    }\n",
        "    llm_methods = {\n",
        "        \"Basic_LLM\": llm_summary,\n",
        "        \"Map_Reduce\": map_reduce_summary,\n",
        "        \"Iterative_Refinement\": iterative_refinement_summary,\n",
        "    }\n",
        "\n",
        "    if summarization_type == \"Extractive\":\n",
        "        summarizer_function = extractive_methods.get(method)\n",
        "    elif summarization_type == \"Abstractive\":\n",
        "        summarizer_function = abstractive_methods.get(method)\n",
        "    elif summarization_type == \"LLM\":\n",
        "        summarizer_function = llm_methods.get(method)\n",
        "    else:\n",
        "        return \"Invalid summarization type\"\n",
        "\n",
        "    if summarizer_function is None:\n",
        "        return \"Invalid method\"\n",
        "\n",
        "    try:\n",
        "        summary = summarizer_function(input_text, word_limit)\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "def summarize_pdf(pdf_file_path, summarization_type, method, word_limit):\n",
        "    try:\n",
        "        loader = PyPDFLoader(pdf_file_path)\n",
        "        docs = loader.load_and_split()\n",
        "        full_text = \" \".join([doc.page_content for doc in docs])\n",
        "        full_text_cleaned = clean_text(full_text)\n",
        "        summary = summarize(full_text_cleaned, summarization_type, method, word_limit)\n",
        "        return summary,full_text_cleaned\n",
        "    except Exception as e:\n",
        "        return f\"Error processing PDF: {e}\"\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove extra newlines and spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def run_summarization(input_text, pdf_file, method, summarization_type, limit):\n",
        "    if input_text:\n",
        "        summary = summarize(input_text, summarization_type, method, word_limit=int(limit) if limit else None)\n",
        "        input_word_count = len(input_text.split())\n",
        "    elif pdf_file:\n",
        "        summary, full_text_cleaned = summarize_pdf(pdf_file.name, summarization_type, method, word_limit=int(limit) if limit else None)\n",
        "        input_word_count = len(full_text_cleaned.split())\n",
        "    else:\n",
        "        summary = \"No input provided.\"\n",
        "\n",
        "    summary_word_count = len(summary.split())\n",
        "    final_summary = f\"{summary}\\n\\nInput Words: {input_word_count}\\nSummary Words: {summary_word_count}\"\n",
        "    return final_summary"
      ],
      "metadata": {
        "id": "TWTU4jw8jkvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio Interface\n",
        "import gradio as gr\n",
        "import re\n",
        "\n",
        "def toggle_input_fields(selection):\n",
        "    if selection == \"Input Text\":\n",
        "        return gr.update(visible=True, value=\"\"), gr.update(visible=False, value=None)\n",
        "    elif selection == \"Upload PDF\":\n",
        "        return gr.update(visible=False, value=\"\"), gr.update(visible=True, value=None)\n",
        "    return gr.update(visible=False, value=\"\"), gr.update(visible(False), value=None)\n",
        "\n",
        "css = \"\"\"\n",
        "h1 {\n",
        "    margin-top: 2rem;\n",
        "    font-size: 2rem;\n",
        "    text-align: center;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=css) as demo:\n",
        "    gr.Markdown(\"<h1 style='text-align:center;'>Document Summarizer App</h1>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            input_method = gr.Radio([\"Input Text\", \"Upload PDF\"], label=\"Select Input Method\")\n",
        "\n",
        "            input_text = gr.Textbox(label=\"Input Text\", placeholder=\"Enter text to summarize\", lines=10, visible=False)\n",
        "            input_pdf = gr.File(label=\"Upload PDF file\", visible=False)\n",
        "\n",
        "            input_method.change(\n",
        "                fn=toggle_input_fields,\n",
        "                inputs=[input_method],\n",
        "                outputs=[input_text, input_pdf]\n",
        "            )\n",
        "\n",
        "            with gr.Tabs():\n",
        "                with gr.TabItem(\"Extractive\"):\n",
        "                    extractive_methods = gr.Dropdown(\n",
        "                        [\"Frequency-based\", \"Luhn\", \"LSA\", \"LexRank\"], label=\"Select Extractive Method\"\n",
        "                    )\n",
        "                    summarize_button_extractive = gr.Button(\"Summarize\", variant=\"primary\")\n",
        "\n",
        "                with gr.TabItem(\"Abstractive\"):\n",
        "                    abstractive_methods = gr.Dropdown(\n",
        "                        [\"T5\", \"BART\" ], label=\"Select Abstractive Method\"\n",
        "                    )\n",
        "                    summarize_button_abstractive = gr.Button(\"Summarize\", variant=\"primary\")\n",
        "                with gr.TabItem(\"LLM\"):\n",
        "                    llm_methods = gr.Dropdown(\n",
        "                        [\"Basic_LLM\",\"Map_Reduce\", \"Iterative_Refinement\"], label=\"Select LLM Method\"\n",
        "                    )\n",
        "                    summarize_button_llm = gr.Button(\"Summarize\", variant=\"primary\")\n",
        "\n",
        "            word_limit = gr.Slider(minimum=10, maximum=1000, step=1, value=100, label=\"Word Limit\")\n",
        "\n",
        "        with gr.Column():\n",
        "            output_summary = gr.Textbox(label=\"Generated Summary\", placeholder=\"The summary will appear here\", lines=20)\n",
        "# summarize button logic\n",
        "    summarize_button_extractive.click(\n",
        "        fn=lambda text, pdf, method, limit: run_summarization(text, pdf, method, \"Extractive\", limit),\n",
        "        inputs=[input_text, input_pdf, extractive_methods, word_limit],\n",
        "        outputs=output_summary\n",
        "    )\n",
        "\n",
        "    summarize_button_abstractive.click(\n",
        "        fn=lambda text, pdf, method, limit: run_summarization(text, pdf, method, \"Abstractive\", limit),\n",
        "        inputs=[input_text, input_pdf, abstractive_methods, word_limit],\n",
        "        outputs=output_summary\n",
        "    )\n",
        "\n",
        "    summarize_button_llm.click(\n",
        "        fn=lambda text, pdf, method, limit: run_summarization(text, pdf, method, \"LLM\", limit),\n",
        "        inputs=[input_text, input_pdf, llm_methods, word_limit],\n",
        "        outputs=output_summary\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "jjRnazwSs66L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "cddd1706-817d-482f-ec9c-815c040a27a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ac3ecc5d1a55ed3a21.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ac3ecc5d1a55ed3a21.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}