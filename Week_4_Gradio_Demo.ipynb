{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUaoq+cf7/+IvlbHVWCRcv"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "oTzhYj-fk-GM",
        "outputId": "c8115f37-aaec-4e40-836d-d71bda65e7af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRunning Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://9da8de6743d770971d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9da8de6743d770971d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install gradio --quiet\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "# Define your summarization functions\n",
        "def extractive_summarization_frequency(txt):\n",
        "    import nltk\n",
        "    import heapq\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "    def summarize_text(text, n):\n",
        "        # Tokenize sentences\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        # Preprocess text to filter out non-alphabetic words and stopwords\n",
        "        def preprocess_text(text):\n",
        "            processed_words = []\n",
        "            for word in word_tokenize(text):\n",
        "                if word.isalpha():\n",
        "                    processed_words.append(word.lower())\n",
        "            return processed_words\n",
        "\n",
        "        words = preprocess_text(text)\n",
        "\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "        # Calculate word frequencies\n",
        "        word_frequencies = {}\n",
        "        for word in filtered_words:\n",
        "            if word in word_frequencies:\n",
        "                word_frequencies[word] += 1\n",
        "            else:\n",
        "                word_frequencies[word] = 1\n",
        "\n",
        "        # Normalize word frequencies\n",
        "        max_frequency = max(word_frequencies.values())\n",
        "        for word in word_frequencies:\n",
        "            word_frequencies[word] /= max_frequency\n",
        "\n",
        "        # Score sentences based on word frequencies\n",
        "        sentence_scores = {}\n",
        "        for sentence in sentences:\n",
        "            sentence_words = preprocess_text(sentence)\n",
        "            for word in sentence_words:\n",
        "                if word in word_frequencies:\n",
        "                    if len(sentence.split(' ')) < 30:  # Only consider sentences with fewer than 30 words\n",
        "                        if sentence in sentence_scores:\n",
        "                            sentence_scores[sentence] += word_frequencies[word]\n",
        "                        else:\n",
        "                            sentence_scores[sentence] = word_frequencies[word]\n",
        "\n",
        "        # Get the top 'n' sentences with the highest scores\n",
        "        summary = heapq.nlargest(n, sentence_scores, key=sentence_scores.get)\n",
        "\n",
        "        return \" \".join(summary)  # Return summary as a string\n",
        "\n",
        "    # Take the full text input from the user\n",
        "    text = txt\n",
        "\n",
        "    # Replace any user-specified \"\\n\" with actual line breaks\n",
        "    text = text.replace(\"\\n\", \"\\n\")\n",
        "\n",
        "    # Take the number of lines for the summary\n",
        "    n = 2\n",
        "\n",
        "    # Summarize the text\n",
        "    return summarize_text(text, n)\n",
        "\n",
        "\n",
        "def extractive_summarization_tfidf(txt):\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from nltk.tokenize import sent_tokenize\n",
        "    import numpy as np\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "\n",
        "    def summarize_text(text, n):\n",
        "        # Tokenize sentences\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        # Generate the TF-IDF matrix\n",
        "        tfidf = TfidfVectorizer()\n",
        "        tfidf_matrix = tfidf.fit_transform(sentences)\n",
        "\n",
        "        # Calculate sentence scores by summing TF-IDF values for each sentence\n",
        "        sentence_scores = np.sum(tfidf_matrix.toarray(), axis=1)\n",
        "\n",
        "        # Get indices of top 'n' sentences with the highest scores\n",
        "        top_sentence_indices = np.argsort(sentence_scores)[-n:]\n",
        "\n",
        "        # Create the summary with the selected sentences\n",
        "        summary = [sentences[i] for i in top_sentence_indices]\n",
        "\n",
        "        return \" \".join(summary)  # Return summary as a string\n",
        "\n",
        "    # Take the full text input from the user\n",
        "    text = txt\n",
        "\n",
        "    # Replace any user-specified \"\\n\" with actual line breaks\n",
        "    text = text.replace(\"\\n\", \"\\n\")\n",
        "\n",
        "    # Take the number of lines for the summary\n",
        "    n = 2\n",
        "\n",
        "    # Summarize the text\n",
        "    return summarize_text(text, n)\n",
        "\n",
        "\n",
        "def extractive_summarization_lsa(txt):\n",
        "    from sumy.parsers.plaintext import PlaintextParser\n",
        "    from sumy.nlp.tokenizers import Tokenizer\n",
        "    from sumy.summarizers.lsa import LsaSummarizer\n",
        "\n",
        "    def summarize_text(text, n):\n",
        "        # Parse the text\n",
        "        parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "\n",
        "        # Initialize LSA summarizer\n",
        "        summarizer = LsaSummarizer()\n",
        "\n",
        "        # Generate summary\n",
        "        summary = summarizer(parser.document, n)\n",
        "\n",
        "        return \" \".join(str(sentence) for sentence in summary)  # Return summary as a string\n",
        "\n",
        "    # Take the full text input from the user\n",
        "    text = txt\n",
        "\n",
        "    # Replace any user-specified \"\\n\" with actual line breaks\n",
        "    text = text.replace(\"\\n\", \"\\n\")\n",
        "\n",
        "    # Take the number of lines for the summary\n",
        "    n = 2\n",
        "\n",
        "    # Summarize the text\n",
        "    return summarize_text(text, n)\n",
        "\n",
        "\n",
        "def abstractive_summarization_bart(text):\n",
        "    from transformers import pipeline\n",
        "\n",
        "    # Initialize the BART summarization pipeline\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "    # The article to summarize\n",
        "    ARTICLE = text\n",
        "\n",
        "    # Generate the summary\n",
        "    summary = summarizer(ARTICLE, max_length=53, min_length=30, do_sample=False)\n",
        "\n",
        "    # Extract the summary text from the output\n",
        "    summary_text = summary[0]['summary_text']\n",
        "\n",
        "    return summary_text  # Return summary as a string\n",
        "\n",
        "\n",
        "def abstractive_summarization_llm(txt):\n",
        "    import os\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")  # Replace  with your Google api key\n",
        "\n",
        "    # Ensure the necessary libraries are installed\n",
        "    !pip install --upgrade --quiet langchain langchain-google-genai beautifulsoup4\n",
        "\n",
        "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "    from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "    # Generalized function to load LLM (Gemini Models)\n",
        "    def load_llm(model=\"gemini-1.5-pro\"):\n",
        "        llm = ChatGoogleGenerativeAI(\n",
        "            model=model,\n",
        "            temperature=0,\n",
        "            max_tokens=None,\n",
        "            timeout=None,\n",
        "            max_retries=2\n",
        "        )\n",
        "        return llm\n",
        "\n",
        "    # Generalized function to get a prompt template\n",
        "    def get_prompt_template():\n",
        "        # Define prompt\n",
        "        prompt = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\"system\", \"Write a concise summary of the following in {num_words} words:\\n\\n\"),\n",
        "                (\"human\", \"{context}\")\n",
        "            ]\n",
        "        )\n",
        "        return prompt\n",
        "\n",
        "    # Function to summarize text using Google Gemini Models\n",
        "    def summarize_text(text, num_words=50, model=\"gemini-1.5-pro\"):\n",
        "        llm = load_llm(model)\n",
        "        prompt = get_prompt_template()\n",
        "        chain = prompt | llm\n",
        "\n",
        "        result = chain.invoke({\n",
        "            \"context\": text,\n",
        "            \"num_words\": num_words\n",
        "        })\n",
        "\n",
        "        return result.content  # Return the summarized result\n",
        "\n",
        "    # Example text for summarization\n",
        "    text = txt\n",
        "\n",
        "    # Specify the number of words for the summary\n",
        "    summary = summarize_text(text, num_words=20, model=\"gemini-1.5-flash\")\n",
        "\n",
        "    return summary  # Return summary as a string\n",
        "\n",
        "\n",
        "def abstractive_summarization_t5(txt):\n",
        "    from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "    # Load the pre-trained T5 model and tokenizer from Hugging Face\n",
        "    model_name = \"t5-small\"\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def summarize_text(text, max_length=150, min_length=40, num_beams=4):\n",
        "        # Prepend \"summarize:\" to the input text\n",
        "        input_text = \"summarize: \" + text\n",
        "\n",
        "        # Tokenize the input text\n",
        "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "        # Generate the summary (using beam search for improved quality)\n",
        "        summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length,\n",
        "                                    length_penalty=2.0, num_beams=num_beams, early_stopping=True)\n",
        "\n",
        "        # Decode the generated tokens into text\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        return summary  # Return summary as a string\n",
        "\n",
        "    summary = summarize_text(txt)\n",
        "\n",
        "    return summary  # Return summary as a string\n",
        "\n",
        "\n",
        "def extractive_summarize_text(text, method):\n",
        "    if method == \"Extractive LSA\":\n",
        "        return extractive_summarization_lsa(text)\n",
        "    elif method == \"Extractive TFIDF\":\n",
        "        return extractive_summarization_tfidf(text)\n",
        "    elif method == \"Extractive FREQUENCY\":\n",
        "        return extractive_summarization_frequency(text)\n",
        "    else:\n",
        "        return \"Please select a valid summarization method.\"\n",
        "\n",
        "\n",
        "def abstractive_summarize_text(text, method):\n",
        "    if method == \"Abstractive BART\":\n",
        "        return abstractive_summarization_bart(text)\n",
        "    elif method == \"Abstractive LLM\":\n",
        "        return abstractive_summarization_llm(text)\n",
        "    elif method == \"Abstractive T5\":\n",
        "        return abstractive_summarization_t5(text)\n",
        "    else:\n",
        "        return \"Please select a valid summarization method.\"\n",
        "\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "css = \"\"\"\n",
        "h1 {\n",
        "    margin-top: 2rem;\n",
        "    font-size: 2rem;\n",
        "    text-align: center;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "input_text = gr.Text(label=\"Input Text\", lines=10)\n",
        "\n",
        "with gr.Blocks(title=\"Summarizer App\", css=css) as demo:\n",
        "    gr.Markdown(\"# Summarizer App\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Extractive\"):\n",
        "            gr.Interface(fn=extractive_summarize_text,\n",
        "                        inputs=[input_text,gr.Dropdown(choices=[\"Extractive LSA\", \"Extractive FREQUENCY\", \"Extractive TFIDF\"],label=\"Select Method\")],\n",
        "                        outputs=['text'],\n",
        "                        flagging_mode='never',\n",
        "                        submit_btn='Generate')\n",
        "        with gr.TabItem(\"Abstractive\"):\n",
        "          gr.Interface(fn=abstractive_summarize_text,\n",
        "                      inputs=[input_text,gr.Dropdown(choices=[ \"Abstractive BART\", \"Abstractive LLM\", \"Abstractive T5\"],label=\"Select Method\")],\n",
        "                      outputs=['text'],\n",
        "                      flagging_mode='never',\n",
        "                      submit_btn='Generate')\n",
        "\n",
        "\n",
        "#demo.launch(server_name='localhost', server_port='8080')\n",
        "\n",
        "demo.launch()"
      ]
    }
  ]
}